{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93fc5cad-89ee-405b-8d43-d6305f8488fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#The five types of plays are classified by their riskiness.\n",
    "#It helps to identify the tactics of the AI\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TGVD_GenericQuery\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "path_training = \"CardsParquetData/trained_blackjack.parquet\"\n",
    "path_match = \"CardsParquetData/played_blackjack.parquet\"\n",
    "\n",
    "df_train = spark.read.parquet(path_training)\n",
    "df_play = spark.read.parquet(path_match)\n",
    "df_train = df_train.withColumn(\"index\", (monotonically_increasing_id() + 1))\n",
    "df_play = df_play.withColumn(\"index\", (monotonically_increasing_id() + 1))\n",
    "\n",
    "count_play = df_play.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fc00ac2-d7bd-477c-9197-1ef9a097e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recycled part from query 1\n",
    "#-----------------------------------------------------\n",
    "#Using the dataset where the model has been trained\n",
    "#-----------------------------------------------------\n",
    "CHUNK_SIZE = 500\n",
    "from pyspark.sql.functions import floor\n",
    "df_clean = df_train.select(\n",
    "    col(\"index\"),\n",
    "    col(\"Shown_cards\").alias(\"Hand_-1\"),\n",
    "    col(\"Hand 0\").alias(\"Hand_0\"),\n",
    "    col(\"Hand 1\").alias(\"Hand_1\"),\n",
    "    col(\"Hand 2\").alias(\"Hand_2\"),\n",
    "    col(\"Hand 3\").alias(\"Hand_3\"),\n",
    "    col(\"Hand 4\").alias(\"Hand_4\"),\n",
    "    col(\"Hand 5\").alias(\"Hand_5\"),\n",
    "    col(\"Hand 6\").alias(\"Hand_6\"),\n",
    "    col(\"Hand 7\").alias(\"Hand_7\")\n",
    ")\n",
    "df_chunks = df_clean.withColumn(\"Chunk Number\", floor(col(\"index\")/CHUNK_SIZE))\n",
    "from pyspark.sql.functions import expr, when, coalesce\n",
    "hand_cols = [\"Hand_0\", \"Hand_1\", \"Hand_2\", \"Hand_3\", \"Hand_4\", \"Hand_5\", \"Hand_6\", \"Hand_7\"]\n",
    "df_moves = df_chunks.withColumn(\"n_moves\",\n",
    "    1 + sum([\n",
    "        when((col(f\"Hand_{i-1}\")[0].isNotNull()) & (col(f\"Hand_{i-1}\")[0] != col(f\"Hand_{i-2}\")[0]), 1).otherwise(0)\n",
    "        for i in range(1, len(hand_cols))\n",
    "    ])\n",
    ")\n",
    "rev_hand_cols = [\"Hand_7\", \"Hand_6\", \"Hand_5\", \"Hand_4\", \"Hand_3\", \"Hand_2\", \"Hand_1\", \"Hand_0\"]\n",
    "df_result_train = df_moves.withColumn(\"Final_Hand\", coalesce(*[col(c) for c in rev_hand_cols]))\n",
    "df_result_train = df_result_train.withColumn(\n",
    "    \"Result\",\n",
    "    when(col(\"Final_Hand\").isNull(), \"Unknown\")\n",
    "    .when(col(\"Final_Hand\")[0] > 21, \"Lose\")\n",
    "    .when(col(\"Final_Hand\")[1] > 21, \"Win\")\n",
    "    .when(col(\"Final_Hand\")[0] > col(\"Final_Hand\")[1], \"Win\")\n",
    "    .when(col(\"Final_Hand\")[0] < col(\"Final_Hand\")[1], \"Lose\")\n",
    "    .otherwise(\"Draw\")\n",
    ")\n",
    "probabilities = [0.25, 0.5, 0.75]\n",
    "quartiles = df_result_train.select(\"n_moves\").approxQuantile(\"n_moves\", probabilities, 0.01)\n",
    "df_risk_train = df_moves.withColumn(\n",
    "    \"Risk Level\",\n",
    "    when(col(\"n_moves\") <= quartiles[0], \"Safe\")\n",
    "    .when((col(\"n_moves\") > quartiles[0]) & (col(\"n_moves\") <= quartiles[1]), \"Tactical\")\n",
    "    .when((col(\"n_moves\") > quartiles[1]) & (col(\"n_moves\") <= quartiles[2]), \"Risky\")\n",
    "    .otherwise(\"Suicidal\")\n",
    ")\n",
    "\n",
    "#-----------------------------------------------------\n",
    "#Using the dataset of the model playing against itself\n",
    "#-----------------------------------------------------\n",
    "df_clean = df_play.select(\n",
    "    col(\"index\"),\n",
    "    col(\"Shown_cards\").alias(\"Hand_-1\"),\n",
    "    col(\"Hand 0\").alias(\"Hand_0\"),\n",
    "    col(\"Hand 1\").alias(\"Hand_1\"),\n",
    "    col(\"Hand 2\").alias(\"Hand_2\"),\n",
    "    col(\"Hand 3\").alias(\"Hand_3\"),\n",
    "    col(\"Hand 4\").alias(\"Hand_4\"),\n",
    "    col(\"Hand 5\").alias(\"Hand_5\")\n",
    ")\n",
    "from pyspark.sql.functions import expr, when, coalesce\n",
    "hand_cols = [\"Hand_0\", \"Hand_1\", \"Hand_2\", \"Hand_3\", \"Hand_4\", \"Hand_5\"]\n",
    "df_moves = df_clean.withColumn(\"n_moves\",\n",
    "    1 + sum([\n",
    "        when((col(f\"Hand_{i-1}\")[0].isNotNull()) & (col(f\"Hand_{i-1}\")[0] != col(f\"Hand_{i-2}\")[0]), 1).otherwise(0)\n",
    "        for i in range(1, len(hand_cols))\n",
    "    ])\n",
    ")\n",
    "rev_hand_cols = [\"Hand_5\", \"Hand_4\", \"Hand_3\", \"Hand_2\", \"Hand_1\", \"Hand_0\"]\n",
    "df_result_train = df_moves.withColumn(\"Final_Hand\", coalesce(*[col(c) for c in rev_hand_cols]))\n",
    "df_result_train = df_result_train.withColumn(\n",
    "    \"Result\",\n",
    "    when(col(\"Final_Hand\").isNull(), \"Unknown\")\n",
    "    .when(col(\"Final_Hand\")[0] > 21, \"Lose\")\n",
    "    .when(col(\"Final_Hand\")[1] > 21, \"Win\")\n",
    "    .when(col(\"Final_Hand\")[0] > col(\"Final_Hand\")[1], \"Win\")\n",
    "    .when(col(\"Final_Hand\")[0] < col(\"Final_Hand\")[1], \"Lose\")\n",
    "    .otherwise(\"Draw\")\n",
    ")\n",
    "probabilities = [0.25, 0.5, 0.75]\n",
    "quartiles = df_result_train.select(\"n_moves\").approxQuantile(\"n_moves\", probabilities, 0.01)\n",
    "df_risk_play = df_moves.withColumn(\n",
    "    \"Risk Level\",\n",
    "    when(col(\"n_moves\") <= quartiles[0], \"Safe\")\n",
    "    .when((col(\"n_moves\") > quartiles[0]) & (col(\"n_moves\") <= quartiles[1]), \"Tactical\")\n",
    "    .when((col(\"n_moves\") > quartiles[1]) & (col(\"n_moves\") <= quartiles[2]), \"Risky\")\n",
    "    .otherwise(\"Suicidal\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de1e0469-31ff-45e2-8293-effe35112580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+----------+\n",
      "|Chunk Number|          Risk Level|count|Proportion|\n",
      "+------------+--------------------+-----+----------+\n",
      "|           0|Why would you ris...|   16|       3.2|\n",
      "|           0|               Risky|   90|      18.0|\n",
      "|           0|                Safe|  393|      78.6|\n",
      "|           1|Why would you ris...|   41|       8.2|\n",
      "|           1|               Risky|  120|      24.0|\n",
      "|           1|                Safe|  339|      67.8|\n",
      "|           2|Why would you ris...|   29|       5.8|\n",
      "|           2|               Risky|  167|      33.4|\n",
      "|           2|                Safe|  304|      60.8|\n",
      "|           3|Why would you ris...|   35|       7.0|\n",
      "|           3|               Risky|  177|      35.4|\n",
      "|           3|                Safe|  288|      57.6|\n",
      "|           4|               Risky|    1|       0.2|\n",
      "+------------+--------------------+-----+----------+\n",
      "\n",
      "+-------------------------------------+-----+----------+\n",
      "|Risk Level                           |count|Proportion|\n",
      "+-------------------------------------+-----+----------+\n",
      "|Why would you risk yourself so much?!|76   |7.6       |\n",
      "|Risky                                |321  |32.1      |\n",
      "|Safe                                 |603  |60.3      |\n",
      "+-------------------------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/16 20:37:09 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.255.255.254:43001 in 10000 milliseconds\n",
      "25/05/16 20:37:09 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1219)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:48)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "#--------------\n",
    "#   QUERY 2\n",
    "#--------------\n",
    "\n",
    "#The types of strategies, (safe, tactical, risky…) and it’s proportion\n",
    "#It helps to identify which strategy is the common one.\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "df_fin = df_risk_train.select(col(\"Chunk Number\"), col(\"n_moves\"), col(\"Risk Level\")).groupBy(\"Chunk Number\", \"Risk Level\").count().orderBy(\"Chunk Number\", \"count\")\n",
    "df_query2 = df_fin.withColumn(\"Proportion\", round(col(\"count\")*100/CHUNK_SIZE, 2))\n",
    "df_query2.show()\n",
    "\n",
    "df_fin = df_risk_play.select(col(\"n_moves\"), col(\"Risk Level\")).groupBy(\"Risk Level\").count().orderBy(\"count\")\n",
    "df_query2 = df_fin.withColumn(\"Proportion\", round(col(\"count\")*100/count_play, 2))\n",
    "df_query2.show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
