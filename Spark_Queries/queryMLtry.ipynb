{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63918d2e-aa0b-467e-b7d9-164a2bc815db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 18:00:28 WARN Utils: Your hostname, ASUS-DIEGO resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/17 18:00:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/17 18:00:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/17 18:00:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/17 18:00:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/17 18:00:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/17 18:00:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "|          Timestamp|Shown_cards|  Hand 0|  Hand 1|  Hand 2|  Hand 3|  Hand 4|Hand 5|index|\n",
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "|2025-05-12 19:25:11|    [10, 1]| [14, 3]|[19, 15]|[19, 21]|[19, 21]|    NULL|  NULL|    1|\n",
      "|2025-05-12 19:25:11|     [4, 5]| [8, 13]|[15, 15]|[15, 26]|    NULL|    NULL|  NULL|    2|\n",
      "|2025-05-12 19:25:11|    [12, 3]|[12, 14]|[12, 19]|[12, 15]|[12, 18]|[12, 26]|  NULL|    3|\n",
      "|2025-05-12 19:25:11|   [11, 12]|[21, 20]|[21, 20]|    NULL|    NULL|    NULL|  NULL|    4|\n",
      "|2025-05-12 19:25:11|    [10, 3]|[10, 12]|[10, 14]|[10, 17]|[10, 22]|    NULL|  NULL|    5|\n",
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We will try to evaluate the game decisions of the model by using FPGrowth, BinaryClassificationEvaluator and Cross Validation.\n",
    "With its results, we will take a look at the frequent decisions made by the model in each case and evaluate if those decisions are being done correctly or if the model is taking unnecessary risks.\n",
    "\n",
    "We will also use the Random Forest implementation from Spark MLlib in order to make game decisions.\n",
    "Once we have this model, we will compare its results to the ones from the deep learning model to check which one gives better results based on its resources.\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TGVD_GenericQuery\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "path_match = \"CardsParquetData/played_blackjack.parquet\"\n",
    "\n",
    "df_play = spark.read.parquet(path_match)\n",
    "df_play = df_play.withColumn(\"index\", (monotonically_increasing_id() + 1))\n",
    "\n",
    "df_play.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f5265b-451d-4abf-a20d-f044105f57dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+--------+--------+--------+------+----------+-----------+-----+\n",
      "|Shown_cards|  Hand 0|  Hand 1|  Hand 2|  Hand 3|  Hand 4|Hand 5|Agents_1st|Dealers_1st|index|\n",
      "+-----------+--------+--------+--------+--------+--------+------+----------+-----------+-----+\n",
      "|    [10, 1]| [14, 3]|[19, 15]|[19, 21]|[19, 21]|    NULL|  NULL|        10|          1|    1|\n",
      "|     [4, 5]| [8, 13]|[15, 15]|[15, 26]|    NULL|    NULL|  NULL|         4|          5|    2|\n",
      "|    [12, 3]|[12, 14]|[12, 19]|[12, 15]|[12, 18]|[12, 26]|  NULL|        12|          3|    3|\n",
      "|   [11, 12]|[21, 20]|[21, 20]|    NULL|    NULL|    NULL|  NULL|        11|         12|    4|\n",
      "|    [10, 3]|[10, 12]|[10, 14]|[10, 17]|[10, 22]|    NULL|  NULL|        10|          3|    5|\n",
      "|   [10, 10]|[10, 18]|[10, 30]|    NULL|    NULL|    NULL|  NULL|        10|         10|    6|\n",
      "|     [8, 4]|[19, 14]|[19, 14]|    NULL|    NULL|    NULL|  NULL|         8|          4|    7|\n",
      "|    [11, 9]|[19, 21]|[19, 21]|    NULL|    NULL|    NULL|  NULL|        11|          9|    8|\n",
      "|     [7, 3]|[15, 13]|[15, 24]|    NULL|    NULL|    NULL|  NULL|         7|          3|    9|\n",
      "|     [8, 7]| [8, 16]|[12, 28]|    NULL|    NULL|    NULL|  NULL|         8|          7|   10|\n",
      "+-----------+--------+--------+--------+--------+--------+------+----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sep = df_play.withColumn(\"Agents_1st\", col(\"Shown_cards\")[0]).withColumn(\"Dealers_1st\", col(\"Shown_cards\")[1]).select(\"Shown_cards\", \"Hand 0\", \"Hand 1\", \"Hand 2\", \"Hand 3\", \"Hand 4\", \"Hand 5\", \"Agents_1st\", \"Dealers_1st\", \"index\")\n",
    "df_sep.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfdfde4-e574-4c71-98c8-9dd5689fcc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|items          |\n",
      "+---------------+\n",
      "|[10, 1, hit]   |\n",
      "|[14, 1, hit]   |\n",
      "|[19, 1, stand] |\n",
      "|[4, 5, hit]    |\n",
      "|[8, 5, hit]    |\n",
      "|[15, 5, stand] |\n",
      "|[12, 3, stand] |\n",
      "|[11, 12, hit]  |\n",
      "|[21, 12, stand]|\n",
      "|[10, 3, stand] |\n",
      "|[10, 10, stand]|\n",
      "|[8, 4, hit]    |\n",
      "|[19, 4, stand] |\n",
      "|[11, 9, hit]   |\n",
      "|[19, 9, stand] |\n",
      "|[7, 3, hit]    |\n",
      "|[15, 3, stand] |\n",
      "|[8, 7, stand]  |\n",
      "|[6, 10, hit]   |\n",
      "|[12, 10, stand]|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------------+\n",
      "|features                     |\n",
      "+-----------------------------+\n",
      "|[player_10, dealer_1, hit]   |\n",
      "|[player_14, dealer_1, hit]   |\n",
      "|[player_19, dealer_1, stand] |\n",
      "|[player_4, dealer_5, hit]    |\n",
      "|[player_8, dealer_5, hit]    |\n",
      "|[player_15, dealer_5, stand] |\n",
      "|[player_12, dealer_3, stand] |\n",
      "|[player_11, dealer_12, hit]  |\n",
      "|[player_21, dealer_12, stand]|\n",
      "|[player_10, dealer_3, stand] |\n",
      "|[player_10, dealer_10, stand]|\n",
      "|[player_8, dealer_4, hit]    |\n",
      "|[player_19, dealer_4, stand] |\n",
      "|[player_11, dealer_9, hit]   |\n",
      "|[player_19, dealer_9, stand] |\n",
      "|[player_7, dealer_3, hit]    |\n",
      "|[player_15, dealer_3, stand] |\n",
      "|[player_8, dealer_7, stand]  |\n",
      "|[player_6, dealer_10, hit]   |\n",
      "|[player_12, dealer_10, stand]|\n",
      "+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "hand_cols = [\"Shown_cards\", \"Hand 0\", \"Hand 1\", \"Hand 2\", \"Hand 3\", \"Hand 4\", \"Hand 5\"]\n",
    "\n",
    "def extract_hand_sequence(row):\n",
    "    result = []\n",
    "    dealer_card = row[\"Dealers_1st\"]\n",
    "\n",
    "    hands = []\n",
    "    for col in hand_cols:\n",
    "        hand = row[col]\n",
    "        if hand is not None:\n",
    "            hands.append(hand)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for i in range(len(hands)):\n",
    "        player_total = hands[i][0]\n",
    "\n",
    "        # Look ahead to see if the next total is different\n",
    "        if i + 1 < len(hands):\n",
    "            next_total = hands[i + 1][0]\n",
    "            action = \"hit\" if next_total != player_total else \"stand\"\n",
    "        else:\n",
    "            # No more hands -> last move is stand\n",
    "            action = \"stand\"\n",
    "\n",
    "        # Si el siguiente movimiento no cambia, asumimos que ya se plant√≥\n",
    "        result.append(Row(items=[player_total, dealer_card, action]))\n",
    "\n",
    "        if action == \"stand\":\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "df_rows = df_sep.rdd.flatMap(extract_hand_sequence).toDF()\n",
    "df_rows.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Convertimos los items a strings\n",
    "@udf(ArrayType(StringType()))\n",
    "def stringify_items(items):\n",
    "    return [f\"player_{items[0]}\", f\"dealer_{items[1]}\", f\"{items[2]}\"]\n",
    "\n",
    "df_fpgrowth = df_rows.withColumn(\"features\", stringify_items(col(\"items\")))\n",
    "df_fpgrowth.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fdd51dd-4728-4aa1-b973-09ca7e1bd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "+------------------+----+\n",
      "|             items|freq|\n",
      "+------------------+----+\n",
      "|[player_19, stand]|  89|\n",
      "|[player_21, stand]|  67|\n",
      "| [dealer_6, stand]|  78|\n",
      "|   [dealer_6, hit]|  94|\n",
      "|   [dealer_1, hit]|  82|\n",
      "|   [player_5, hit]|  84|\n",
      "|[player_10, stand]|  74|\n",
      "|  [player_10, hit]|  85|\n",
      "|[player_17, stand]|  56|\n",
      "|  [player_17, hit]|  50|\n",
      "| [dealer_8, stand]| 130|\n",
      "|   [dealer_8, hit]| 159|\n",
      "|[dealer_12, stand]|  80|\n",
      "|  [dealer_12, hit]| 123|\n",
      "|[dealer_11, stand]|  75|\n",
      "|  [dealer_11, hit]|  73|\n",
      "| [dealer_4, stand]|  92|\n",
      "|   [dealer_4, hit]| 111|\n",
      "| [dealer_2, stand]|  67|\n",
      "|   [dealer_2, hit]|  85|\n",
      "|[player_20, stand]|  68|\n",
      "|[player_15, stand]|  63|\n",
      "| [dealer_9, stand]|  59|\n",
      "|   [dealer_9, hit]| 115|\n",
      "| [dealer_3, stand]|  72|\n",
      "|   [dealer_3, hit]|  62|\n",
      "|   [player_6, hit]|  90|\n",
      "|   [player_8, hit]| 126|\n",
      "|   [player_4, hit]|  77|\n",
      "|   [player_9, hit]|  86|\n",
      "+------------------+----+\n",
      "only showing top 30 rows\n",
      "\n",
      "Association Rules:\n",
      "+-----------+----------+------------------+------------------+--------------------+\n",
      "| antecedent|consequent|        confidence|              lift|             support|\n",
      "+-----------+----------+------------------+------------------+--------------------+\n",
      "| [player_6]|     [hit]| 0.989010989010989|1.7745637999807977|  0.0398406374501992|\n",
      "|[player_20]|   [stand]|0.9855072463768116|2.2262608695652175| 0.03010181496237273|\n",
      "| [player_2]|     [hit]|0.9833333333333333| 1.764376489277204| 0.02611775121735281|\n",
      "|[player_21]|   [stand]|0.9710144927536232|2.1935217391304347|0.029659141212926073|\n",
      "| [player_7]|     [hit]|0.9705882352941176| 1.741508199785077|0.043824701195219126|\n",
      "| [player_5]|     [hit]|0.9655172413793104|  1.73240941086248| 0.03718459495351926|\n",
      "| [player_4]|     [hit]|            0.9625|1.7269956314535346| 0.03408587870739265|\n",
      "| [player_3]|     [hit]|0.9428571428571428|1.6917508226483604|0.029216467463479414|\n",
      "|[player_19]|   [stand]|0.9270833333333334|        2.09428125| 0.03939796370075255|\n",
      "|[player_18]|   [stand]|              0.85|1.9201499999999998| 0.03010181496237273|\n",
      "| [player_8]|     [hit]|0.7730061349693251|1.3869903565494086|0.055776892430278883|\n",
      "| [player_9]|     [hit]|0.7678571428571429| 1.377751616929536|0.038069942452412575|\n",
      "|[player_14]|     [hit]|0.7478260869565218|1.3418102704009394|0.038069942452412575|\n",
      "| [dealer_1]|     [hit]|0.6666666666666666|1.1961874503574266| 0.03629924745462594|\n",
      "| [dealer_9]|     [hit]|0.6609195402298851|1.1858754895784833| 0.05090748118636565|\n",
      "|[player_16]|   [stand]|0.6373626373626373|1.4398021978021978|0.025675077467906152|\n",
      "|[player_15]|   [stand]|              0.63|           1.42317|0.027888446215139442|\n",
      "|[dealer_12]|     [hit]|0.6059113300492611|1.0871752935514543| 0.05444887118193891|\n",
      "+-----------+----------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "fp = FPGrowth(itemsCol=\"features\", minSupport=0.02, minConfidence=0.6)\n",
    "model = fp.fit(df_fpgrowth)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "model.freqItemsets.filter(size(col(\"items\")) >= 2).show(30)\n",
    "\n",
    "print(\"Association Rules:\")\n",
    "model.associationRules.orderBy(\"confidence\", ascending = False).show()\n",
    "#Interpretation when the player has 20, in 98% of cases the model stands.\n",
    "#Interpretation when the dealer has 1, in 66% of cases the model hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982f4a30-b1aa-4714-ad06-bdcc447dbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Extraer columnas\n",
    "df_ml = df_rows.withColumn(\"player_total\", col(\"items\")[0].cast(\"int\")) \\\n",
    "               .withColumn(\"dealer_card\", col(\"items\")[1].cast(\"int\")) \\\n",
    "               .withColumn(\"label_str\", col(\"items\")[2])\n",
    "\n",
    "# Codificar acci√≥n como 0/1\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\")\n",
    "df_ml_indexed = indexer.fit(df_ml).transform(df_ml)\n",
    "\n",
    "# Vector de caracter√≠sticas\n",
    "assembler = VectorAssembler(inputCols=[\"player_total\", \"dealer_card\"], outputCol=\"features\")\n",
    "df_final = assembler.transform(df_ml_indexed).select(\"features\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0e595-1ae3-45e1-b0d8-93cadc01b964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.9666481334392376\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.maxDepth, [3, 5, 7]).build()\n",
    "\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "cv_model = cv.fit(df_final)\n",
    "predictions = cv_model.transform(df_final)\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest AUC: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
