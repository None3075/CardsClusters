{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad66c53-9925-43c4-a5c7-db4dcf87aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/16 22:11:59 WARN Utils: Your hostname, ASUS-DIEGO resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/16 22:11:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/16 22:12:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/16 22:12:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/16 22:12:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/16 22:12:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/16 22:12:01 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+--------+--------+--------+------+------+------+------+-----+\n",
      "|          Timestamp|Shown_cards|  Hand 0|  Hand 1|  Hand 2|  Hand 3|Hand 4|Hand 5|Hand 6|Hand 7|index|\n",
      "+-------------------+-----------+--------+--------+--------+--------+------+------+------+------+-----+\n",
      "|2025-05-12 19:24:20|    [10, 4]| [10, 7]|[10, 11]|[10, 21]|[10, 21]|  NULL|  NULL|  NULL|  NULL|    1|\n",
      "|2025-05-12 19:24:20|     [4, 7]|[13, 12]|[23, 21]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|    2|\n",
      "|2025-05-12 19:24:20|   [-1, 10]|[14, 18]|[14, 29]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|    3|\n",
      "|2025-05-12 19:24:20|    [8, 12]|[20, 12]|[22, 12]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|    4|\n",
      "|2025-05-12 19:24:20|     [3, 6]| [3, 15]| [3, 23]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|    5|\n",
      "+-------------------+-----------+--------+--------+--------+--------+------+------+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "|          Timestamp|Shown_cards|  Hand 0|  Hand 1|  Hand 2|  Hand 3|  Hand 4|Hand 5|index|\n",
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "|2025-05-12 19:25:11|    [10, 1]| [14, 3]|[19, 15]|[19, 21]|[19, 21]|    NULL|  NULL|    1|\n",
      "|2025-05-12 19:25:11|     [4, 5]| [8, 13]|[15, 15]|[15, 26]|    NULL|    NULL|  NULL|    2|\n",
      "|2025-05-12 19:25:11|    [12, 3]|[12, 14]|[12, 19]|[12, 15]|[12, 18]|[12, 26]|  NULL|    3|\n",
      "|2025-05-12 19:25:11|   [11, 12]|[21, 20]|[21, 20]|    NULL|    NULL|    NULL|  NULL|    4|\n",
      "|2025-05-12 19:25:11|    [10, 3]|[10, 12]|[10, 14]|[10, 17]|[10, 22]|    NULL|  NULL|    5|\n",
      "+-------------------+-----------+--------+--------+--------+--------+--------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#The chances of taking a card based on the card of the opponent and the current hand.\n",
    "#It help to understand how the model is working\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TGVD_GenericQuery\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "path_training = \"CardsParquetData/trained_blackjack.parquet\"\n",
    "path_match = \"CardsParquetData/played_blackjack.parquet\"\n",
    "\n",
    "df_train = spark.read.parquet(path_training)\n",
    "df_play = spark.read.parquet(path_match)\n",
    "df_train = df_train.withColumn(\"index\", (monotonically_increasing_id() + 1))\n",
    "df_play = df_play.withColumn(\"index\", (monotonically_increasing_id() + 1))\n",
    "\n",
    "df_train.show(5)\n",
    "df_play.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a51244-0f75-4c36-9d9f-f10c3d4b12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "|index| Hand_-1|  Hand_0|  Hand_1|  Hand_2|  Hand_3|Hand_4|Hand_5|Hand_6|Hand_7|Chunk Number|n_hits|\n",
      "+-----+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "|    1| [10, 4]| [10, 7]|[10, 11]|[10, 21]|[10, 21]|  NULL|  NULL|  NULL|  NULL|           0|     1|\n",
      "|    2|  [4, 7]|[13, 12]|[23, 21]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|           0|     3|\n",
      "|    3|[-1, 10]|[14, 18]|[14, 29]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|           0|     2|\n",
      "|    4| [8, 12]|[20, 12]|[22, 12]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|           0|     3|\n",
      "|    5|  [3, 6]| [3, 15]| [3, 23]|    NULL|    NULL|  NULL|  NULL|  NULL|  NULL|           0|     1|\n",
      "+-----+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------+--------+--------+--------+--------+--------+------+------------+------+\n",
      "|index| Hand_-1|  Hand_0|  Hand_1|  Hand_2|  Hand_3|  Hand_4|Hand_5|Chunk Number|n_hits|\n",
      "+-----+--------+--------+--------+--------+--------+--------+------+------------+------+\n",
      "|    1| [10, 1]| [14, 3]|[19, 15]|[19, 21]|[19, 21]|    NULL|  NULL|           0|     3|\n",
      "|    2|  [4, 5]| [8, 13]|[15, 15]|[15, 26]|    NULL|    NULL|  NULL|           0|     3|\n",
      "|    3| [12, 3]|[12, 14]|[12, 19]|[12, 15]|[12, 18]|[12, 26]|  NULL|           0|     1|\n",
      "|    4|[11, 12]|[21, 20]|[21, 20]|    NULL|    NULL|    NULL|  NULL|           0|     2|\n",
      "|    5| [10, 3]|[10, 12]|[10, 14]|[10, 17]|[10, 22]|    NULL|  NULL|           0|     1|\n",
      "+-----+--------+--------+--------+--------+--------+--------+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import floor\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "df_clean = df_train.select(\n",
    "    col(\"index\"),\n",
    "    col(\"Shown_cards\").alias(\"Hand_-1\"),\n",
    "    col(\"Hand 0\").alias(\"Hand_0\"),\n",
    "    col(\"Hand 1\").alias(\"Hand_1\"),\n",
    "    col(\"Hand 2\").alias(\"Hand_2\"),\n",
    "    col(\"Hand 3\").alias(\"Hand_3\"),\n",
    "    col(\"Hand 4\").alias(\"Hand_4\"),\n",
    "    col(\"Hand 5\").alias(\"Hand_5\"),\n",
    "    col(\"Hand 6\").alias(\"Hand_6\"),\n",
    "    col(\"Hand 7\").alias(\"Hand_7\")\n",
    ")\n",
    "df_chunks = df_clean.withColumn(\"Chunk Number\", floor(col(\"index\")/CHUNK_SIZE))\n",
    "\n",
    "from pyspark.sql.functions import expr, when, coalesce\n",
    "\n",
    "hand_cols = [\"Hand_0\", \"Hand_1\", \"Hand_2\", \"Hand_3\", \"Hand_4\", \"Hand_5\", \"Hand_6\", \"Hand_7\"]\n",
    "df_moves_train = df_chunks.withColumn(\"n_hits\",\n",
    "    1 + sum([\n",
    "        when((col(f\"Hand_{i-1}\")[0].isNotNull()) & (col(f\"Hand_{i-1}\")[0] != col(f\"Hand_{i-2}\")[0]), 1).otherwise(0)\n",
    "        for i in range(1, len(hand_cols))\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_moves_train.show(5)\n",
    "\n",
    "df_clean = df_play.select(\n",
    "    col(\"index\"),\n",
    "    col(\"Shown_cards\").alias(\"Hand_-1\"),\n",
    "    col(\"Hand 0\").alias(\"Hand_0\"),\n",
    "    col(\"Hand 1\").alias(\"Hand_1\"),\n",
    "    col(\"Hand 2\").alias(\"Hand_2\"),\n",
    "    col(\"Hand 3\").alias(\"Hand_3\"),\n",
    "    col(\"Hand 4\").alias(\"Hand_4\"),\n",
    "    col(\"Hand 5\").alias(\"Hand_5\")\n",
    ")\n",
    "df_chunks = df_clean.withColumn(\"Chunk Number\", floor(col(\"index\")/CHUNK_SIZE))\n",
    "\n",
    "from pyspark.sql.functions import expr, when, coalesce\n",
    "\n",
    "hand_cols = [\"Hand_0\", \"Hand_1\", \"Hand_2\", \"Hand_3\", \"Hand_4\", \"Hand_5\"]\n",
    "df_moves_play = df_chunks.withColumn(\"n_hits\",\n",
    "    1 + sum([\n",
    "        when((col(f\"Hand_{i-1}\")[0].isNotNull()) & (col(f\"Hand_{i-1}\")[0] != col(f\"Hand_{i-2}\")[0]), 1).otherwise(0)\n",
    "        for i in range(1, len(hand_cols))\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_moves_play.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ecb24f-6d3f-48f9-845f-d887489487bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+------------+-------------+-----------------+-----------+\n",
      "|Agent_1st_Card|Opponent_1st_Card|Chunk Number|Hits_Variance|   Hits_Quartiles|Unique_Hits|\n",
      "+--------------+-----------------+------------+-------------+-----------------+-----------+\n",
      "|            -1|                0|           3|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                1|           1|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                2|           0|          0.0|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                3|           0|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                3|           3|          0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "|            -1|                4|           1|         NULL|  [4.0, 4.0, 4.0]|        [4]|\n",
      "|            -1|                4|           2|         NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                4|           3|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                5|           0|          4.5|[2.75, 3.5, 4.25]|     [5, 2]|\n",
      "|            -1|                5|           1|         NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                5|           2|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                5|           3|          0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "|            -1|                6|           1|          0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "|            -1|                6|           2|         NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                7|           2|         NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                8|           2|          0.0|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                8|           3|          0.0|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                9|           0|         NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|               10|           0|         NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|               10|           1|          0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "+--------------+-----------------+------------+-------------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-----------------+-------------------+-----------------+-----------+\n",
      "|Agent_1st_Card|Opponent_1st_Card|      Hits_Variance|   Hits_Quartiles|Unique_Hits|\n",
      "+--------------+-----------------+-------------------+-----------------+-----------+\n",
      "|            -1|                0|               NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                1|               NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|                2|               NULL|  [4.0, 4.0, 4.0]|        [4]|\n",
      "|            -1|                3|               NULL|  [4.0, 4.0, 4.0]|        [4]|\n",
      "|            -1|                4|                0.0|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|                5|                0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "|            -1|                6|                0.5|[2.25, 2.5, 2.75]|     [2, 3]|\n",
      "|            -1|                7|                0.7|  [3.0, 3.0, 4.0]|  [2, 3, 4]|\n",
      "|            -1|                8|0.33333333333333337|  [2.5, 3.0, 3.0]|     [2, 3]|\n",
      "|            -1|                9|               NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|            -1|               10|               NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|               11|               NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|            -1|               12|               NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|             0|                2|               NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|             0|                4|               NULL|  [2.0, 2.0, 2.0]|        [2]|\n",
      "|             0|                5|               NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|             0|                6|0.25000000000000006| [3.0, 3.0, 3.25]|     [3, 4]|\n",
      "|             0|                8| 0.6666666666666666|[2.75, 3.0, 3.25]|  [2, 3, 4]|\n",
      "|             0|                9|               NULL|  [3.0, 3.0, 3.0]|        [3]|\n",
      "|             0|               10|0.25000000000000006| [3.0, 3.0, 3.25]|     [3, 4]|\n",
      "+--------------+-----------------+-------------------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance\n",
    "\n",
    "'''\n",
    "Variance:\n",
    "    - It helps you know if for a combination of (my card, opponent's card) the number of hits is constant (low variance, consistent decisions) or changes a lot (high variance, doubtful decisions).\n",
    "\n",
    "Percentile:\n",
    "    - You can see if the number of hits is generally low, medium, or high, without depending on the average (which is a restricted function).\n",
    "\n",
    "Collect Set:\n",
    "    - It gives you the different number of hits the model has made for a combination (my card, opponent's card). If there are many different options, it's a sign of inconsistency or exploration.\n",
    "'''\n",
    "\n",
    "df_moves_train = df_moves_train.withColumn(\"Agent_1st_Card\", col(\"Hand_-1\")[0])\n",
    "df_moves_train = df_moves_train.withColumn(\"Opponent_1st_Card\", col(\"Hand_-1\")[1])\n",
    "\n",
    "df_hits_stats = df_moves_train.groupBy(\"Agent_1st_Card\", \"Opponent_1st_Card\", \"Chunk Number\") \\\n",
    "    .agg(\n",
    "        variance(\"n_hits\").alias(\"Hits_Variance\"),\n",
    "        expr(\"percentile(n_hits, array(0.25, 0.5, 0.75)) as Hits_Quartiles\"),\n",
    "        expr(\"collect_set(n_hits) as Unique_Hits\")\n",
    "    )\n",
    "\n",
    "df_hits_stats.show()\n",
    "\n",
    "df_moves_play = df_moves_play.withColumn(\"Agent_1st_Card\", col(\"Hand_-1\")[0])\n",
    "df_moves_play = df_moves_play.withColumn(\"Opponent_1st_Card\", col(\"Hand_-1\")[1])\n",
    "\n",
    "df_stats_play = df_moves_play.groupBy(\"Agent_1st_Card\", \"Opponent_1st_Card\") \\\n",
    "    .agg(\n",
    "        variance(\"n_hits\").alias(\"Hits_Variance\"),\n",
    "        expr(\"percentile(n_hits, array(0.25, 0.5, 0.75)) as Hits_Quartiles\"),\n",
    "        expr(\"collect_set(n_hits) as Unique_Hits\")\n",
    "    )\n",
    "\n",
    "df_stats_play.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
